



### About the dataset:

*   List item

*   List item

*   List item

*   List item
*   List item


*   List item


*   List item


*   List item


1.id:unique id for a news article
2.title:the title of a news article
3.author:author of a news article
4.text:the text of the article;could be incomplete
5.label:a label that marks whether the news article is fake or not
        1.Fake news
        0.Real news

Importing the dependencies

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,roc_curve, auc, classification_report,precision_recall_curve, average_precision_score


from sklearn.model_selection import learning_curve

import nltk
nltk.download('stopwords')


print(stopwords.words('english'))



Data pre-processing



news_dataset=pd.read_csv('/content/train.csv')

news_dataset.shape


news_dataset.head()


news_dataset.isnull().sum()


news_dataset=news_dataset.fillna('')


news_dataset['content']=news_dataset['author']+' '+news_dataset['title']

print(news_dataset['content'])


X=news_dataset.drop(columns='label',axis=1)
Y=news_dataset['label']

print(X)
print(Y)







port_stem=PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content




news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])


X=news_dataset['content'].values
Y=news_dataset['label'].values

print(X)

print(Y)

Y.shape



vectorizer=TfidfVectorizer()
vectorizer.fit(X)
X=vectorizer.transform(X)

print(X)



Splitting the dataset to training & test data


X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=2)



Training the model:Logistic Regression





model=LogisticRegression()


model.fit(X_train,Y_train)

Evaluation
accuracy score



X_train_prediction=model.predict(X_train)
training_data_accuracy=accuracy_score(X_train_prediction,Y_train)

print('Accuracy score of the training data:',training_data_accuracy)


X_test_prediction=model.predict(X_test)
test_data_accuracy=accuracy_score(X_test_prediction,Y_test)

print('Accuracy score of the test data:',test_data_accuracy)



Making a predictive system


X_new=X_test[0]
prediction=model.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

print(Y_test[0])

import joblib


joblib.dump(model, 'fake_news_model.pkl')


joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')



X_train_prediction = model.predict(X_train)

training_accuracy = accuracy_score(Y_train, X_train_prediction)
training_precision = precision_score(Y_train, X_train_prediction)
training_recall = recall_score(Y_train, X_train_prediction)
training_f1 = f1_score(Y_train, X_train_prediction)
training_conf_matrix = confusion_matrix(Y_train, X_train_prediction)

print("\nTraining Data Evaluation:")
print(f"Accuracy: {training_accuracy:.4f}")
print(f"Precision: {training_precision:.4f}")
print(f"Recall: {training_recall:.4f}")
print(f"F1 Score: {training_f1:.4f}")
print("Confusion Matrix:\n", training_conf_matrix)



X_test_prediction = model.predict(X_test)

test_accuracy = accuracy_score(Y_test, X_test_prediction)
test_precision = precision_score(Y_test, X_test_prediction)
test_recall = recall_score(Y_test, X_test_prediction)
test_f1 = f1_score(Y_test, X_test_prediction)
test_conf_matrix = confusion_matrix(Y_test, X_test_prediction)


print("\nTest Data Evaluation:")
print(f"Accuracy: {test_accuracy:.4f}")
print(f"Precision: {test_precision:.4f}")
print(f"Recall: {test_recall:.4f}")
print(f"F1 Score: {test_f1:.4f}")
print("Confusion Matrix:\n", test_conf_matrix)


def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title(title)
    plt.show()
plot_confusion_matrix(training_conf_matrix, "Training Data Confusion Matrix")


plot_confusion_matrix(test_conf_matrix, "Test Data Confusion Matrix")


y_probs = model.predict_proba(X_test)[:, 1]


fpr, tpr, thresholds = roc_curve(Y_test, y_probs)


roc_auc = auc(fpr, tpr)


plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.grid()
plt.show()



report = classification_report(Y_test, X_test_prediction)
print(report)



with open('classification_report.txt', 'w') as f:
    f.write(report)
train_report = classification_report(Y_train, X_train_prediction)
print(train_report)
with open('classification_report.txt', 'w') as f:
    f.write(report)




y_probs = model.predict_proba(X_test)[:, 1]


precision, recall, thresholds = precision_recall_curve(Y_test, y_probs)


average_precision = average_precision_score(Y_test, y_pro
plt.figure(figsize=(8,6))
plt.plot(recall, precision, color='purple', label=f'Average Precision = {average_precision:.4f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.grid()
plt.show()




train_sizes, train_scores, test_scores = learning_curve(
    estimator=model,
    X=X_train,
    y=Y_train,
    train_sizes=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

train_mean = train_scores.mean(axis=1)
train_std = train_scores.std(axis=1)
test_mean = test_scores.mean(axis=1)
test_std = test_scores.std(axis=1)

print("Training Data:")
for i, size in enumerate(train_sizes):
    print(f"Training Size {size*100}%: Mean = {train_mean[i]:.4f}, Std = {train_std[i]:.4f}")

print("\nCross-validation Data:")
for i, size in enumerate(train_sizes):
    print(f"Training Size {size*100}%: Mean = {test_mean[i]:.4f}, Std = {test_std[i]:.4f}")


plt.figure(figsize=(8,6))
plt.plot(train_sizes, train_mean, color='blue', label='Training Accuracy', marker='o')
plt.plot(train_sizes, test_mean, color='green', label='Cross-validation Accuracy', marker='x')

plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.1)
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='green', alpha=0.1)


plt.title('Learning Curve')
plt.xlabel('Training Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()



# Naive Bayes Model

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


nb_model = MultinomialNB()


nb_model.fit(X_train, Y_train)


X_train_pred_nb = nb_model.predict(X_train)
train_accuracy_nb = accuracy_score(Y_train, X_train_pred_nb)
print('Training Accuracy (Naive Bayes):', train_accuracy_nb)


X_test_pred_nb = nb_model.predict(X_test)
test_accuracy_nb = accuracy_score(Y_test, X_test_pred_nb)
print('Test Accuracy (Naive Bayes):', test_accuracy_nb)

# Confusion Matrix
cm_nb = confusion_matrix(Y_test, X_test_pred_nb)
print('Confusion Matrix (Naive Bayes):\n', cm_nb)

# Classification Report
cr_nb = classification_report(Y_test, X_test_pred_nb)
print('Classification Report (Naive Bayes):\n', cr_nb)
import seaborn as sns
import matplotlib.pyplot as plt

# confusion matrix
plt.figure(figsize=(6,4))
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Naive Bayes')
plt.show()


#K-Nearest Neighbors (KNN)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report


knn_model = KNeighborsClassifier(n_neighbors=5)


knn_model.fit(X_train, Y_train)


X_train_pred_knn = knn_model.predict(X_train)
train_accuracy_knn = accuracy_score(Y_train, X_train_pred_knn)
print('Training Accuracy (KNN):', train_accuracy_knn)


X_test_pred_knn = knn_model.predict(X_test)
test_accuracy_knn = accuracy_score(Y_test, X_test_pred_knn)
print('Test Accuracy (KNN):', test_accuracy_knn)

# Confusion Matrix
cm_knn = confusion_matrix(Y_test, X_test_pred_knn)
print('Confusion Matrix (KNN):\n', cm_knn)

# Classification Report
cr_knn = classification_report(Y_test, X_test_pred_knn)
print('Classification Report (KNN):\n', cr_knn)
import seaborn as sns
import matplotlib.pyplot as plt

# confusion matrix for KNN
plt.figure(figsize=(6,4))
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - KNN')
plt.show()


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns


# Logistic Regression evaluation

lr_y_pred = model.predict(X_test)

lr_accuracy = accuracy_score(Y_test, lr_y_pred)
lr_precision = precision_score(Y_test, lr_y_pred)
lr_recall = recall_score(Y_test, lr_y_pred)
lr_f1 = f1_score(Y_test, lr_y_pred)


# Naive Bayes evaluation

from sklearn.naive_bayes import MultinomialNB

nb_model = MultinomialNB()
nb_model.fit(X_train, Y_train)

nb_y_pred = nb_model.predict(X_test)

nb_accuracy = accuracy_score(Y_test, nb_y_pred)
nb_precision = precision_score(Y_test, nb_y_pred)
nb_recall = recall_score(Y_test, nb_y_pred)
nb_f1 = f1_score(Y_test, nb_y_pred)


# KNN evaluation

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, Y_train)

knn_y_pred = knn_model.predict(X_test)

knn_accuracy = accuracy_score(Y_test, knn_y_pred)
knn_precision = precision_score(Y_test, knn_y_pred)
knn_recall = recall_score(Y_test, knn_y_pred)
knn_f1 = f1_score(Y_test, knn_y_pred)


models = ['Logistic Regression', 'Naive Bayes', 'KNN']
accuracy = [lr_accuracy, nb_accuracy, knn_accuracy]
precision = [lr_precision, nb_precision, knn_precision]
recall = [lr_recall, nb_recall, knn_recall]
f1 = [lr_f1, nb_f1, knn_f1]


x = np.arange(len(models))
width = 0.2


sns.set_style('whitegrid')
fig, ax = plt.subplots(figsize=(12,7))

rects1 = ax.bar(x - 1.5*width, accuracy, width, label='Accuracy')
rects2 = ax.bar(x - 0.5*width, precision, width, label='Precision')
rects3 = ax.bar(x + 0.5*width, recall, width, label='Recall')
rects4 = ax.bar(x + 1.5*width, f1, width, label='F1 Score')

ax.set_xlabel('Models', fontsize=14)
ax.set_ylabel('Score', fontsize=14)
ax.set_title('Comparison of Model Performance', fontsize=16)
ax.set_xticks(x)
ax.set_xticklabels(models, fontsize=12)
ax.legend(fontsize=12)

def add_labels(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width()/2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=10)

add_labels(rects1)
add_labels(rects2)
add_labels(rects3)
add_labels(rects4)

plt.ylim(0, 1.1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


print("Logistic Regression --> Accuracy:", lr_accuracy, "Precision:", lr_precision, "Recall:", lr_recall, "F1:", lr_f1)
print("Naive Bayes --> Accuracy:", nb_accuracy, "Precision:", nb_precision, "Recall:", nb_recall, "F1:", nb_f1)
print("KNN --> Accuracy:", knn_accuracy, "Precision:", knn_precision, "Recall:", knn_recall, "F1:", knn_f1)
